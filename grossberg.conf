% true oznaczałoby losowanie wag dla neuronów, weights jest wtedy ignorowane
random_weights = true;

% dane wejściowe
input_data = [

 1 1 1  0 0 0  0 0 0;
 0 0 0  1 1 1  0 0 0;
 0 0 0  0 0 0  1 1 1;

 1 0 0  1 0 0  1 0 0;
 0 1 0  0 1 0  0 1 0;
 0 0 1  0 0 1  0 0 1;

 1 0 0  0 0 0  0 0 0;
 0 0 0  0 0 0  0 0 1;
 0 0 1  0 1 0  1 0 0;
];

expected = [
 1 0 0;
 1 0 0;
 1 0 0;

 0 1 0;
 0 1 0;
 0 1 0;

 0 0 1;
 0 0 1;
 0 0 1;
];

%global epochs = [1 8000 16000 24000]

global layers = {};
learn_steps = 5010;
% pierwsza warstwa
% liczba neuronów w warstwie
layers{1}.type='kohonen';
layers{1}.neurons = 9;
layers{1}.row_count = 1;
% funkcja aktywacji
layers{1}.activation_function =@(X) sigmoid(X);
% wartość bias
layers{1}.bias = 0;
% przedział, z którego są losowane wagi
layers{1}.rand_min = 0.0;
layers{1}.rand_max = 0.2;
layers{1}.neighbourhood_width = [1 0 0 0 ];%[7.0 5.0 3.0 1.0];
layers{1}.conscience_coefficient = [1 0.5 0.25 0]; %[0 0 0 0];%
layers{1}.learning_coefficient = [0.075 0.05 0.02 0];%[1 0.5 0.3 0.1]%
layers{1}.learn = false;
layers{1}.random_weights =true;

layers{1}.weights = [  ]
layers{1}.epochs = [1 1000 2000 5000]


layers{2}.type='grossberg';
layers{2}.neurons=3;
layers{2}.bias = 0;
layers{2}.rand_min = -1.0;
layers{2}.rand_max = 1.0;
layers{2}.activation_function = @(X) linear(X,1,0);
layers{2}.epochs = [5000 10000 15000 20000];
layers{2}.coeffs = [0.8 0.5 0.3 0.1];
layers{2}.epoch=0;
layers{2}.random_weights = true








